{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Actividad extracurricular 12 - Web Scraping\n",
    "\n",
    "**Nombre:** Alexis Bautista  \n",
    "**Fecha de entrega:** 04 de enero de 2025  \n",
    "**Paralelo:** GR1CC  \n",
    "**Enlace de GitHub:** https://github.com/alexis-bautista/Actividad12-MN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping \n",
    "\n",
    "<p style=\"text-align: justify;\">El web scraping, también conocido como extracción de datos web, es el proceso automatizado de recolectar y estructurar información desde sitios web. Este procedimiento es ampliamente utilizado en diversas áreas como la monitorización de precios, investigación de mercados, recopilación de noticias y también en proyectos académicos o de aprendizaje automático (machine learning). En esencia, el web scraping emula la acción de copiar y pegar información manualmente, pero de forma sistemática y a gran escala, permitiendo extraer datos de una o varias páginas web en un tiempo considerablemente menor.</p>\n",
    "\n",
    "### Funcionamiento del Web Scraping\n",
    "\n",
    "En el web scraping intervienen principalmente dos componentes:\n",
    "\n",
    "1. **Crawler (rastreadores web):** Un programa diseñado para navegar por la web de manera automática. Utiliza los enlaces de las páginas web para explorar diferentes sitios y descubrir contenidos de interés. Este rastreador simula la navegación de un usuario humano y realiza solicitudes a servidores web para acceder a los contenidos.\n",
    "\n",
    "2. **Scraper (extractor de datos):** Este componente se encarga de analizar las páginas obtenidas por el crawler, identificar las secciones relevantes y extraer los datos específicos deseados. Por ejemplo, un scraper puede buscar en una tabla de precios o extraer artículos de noticias. Para funcionar correctamente, se debe proporcionar al scraper la URL del sitio de interés y definir las características de los datos que se quieren recolectar.\n",
    "\n",
    "### Herramientas Comunes en Web Scraping\n",
    "\n",
    "<p style =\"text-aling: justify;\"\">El proceso de web scraping puede realizarse utilizando herramientas y frameworks que simplifican tareas como el acceso a sitios web, el análisis del código HTML y la extracción de datos. En Python, existen varias librerias populares que permiten implementar soluciones eficientes de scraping:</p>\n",
    "\n",
    "- **Requests:** Permite realizar solicitudes HTTP para acceder al contenido de las páginas web. Es sencillo de usar y ampliamente utilizado para interactuar con servidores web.\n",
    "\n",
    "- **BeautifulSoup:** Una biblioteca diseñada para analizar código HTML y XML. Facilita la navegación y búsqueda en estructuras jerárquicas, ayudando a localizar y extraer datos específicos.\n",
    "\n",
    "- **Scrapy:** Un framework completo para web scraping que incluye funcionalidades avanzadas como la gestión de crawlers, manejo de datos estructurados y optimización del rendimiento.\n",
    "\n",
    "- **Selenium:** Ideal para interactuar con páginas web dinámicas que utilizan JavaScript. Permite automatizar navegadores web para realizar tareas complejas como completar formularios o hacer clic en botones.\n",
    "\n",
    "### Proceso del Web Scraping\n",
    "\n",
    "El proceso de web scraping puede dividirse en las siguientes etapas:\n",
    "\n",
    "1. **Acceso al sitio web:** Se envía una solicitud HTTP al servidor del sitio web deseado. En respuesta, el servidor proporciona el código HTML de la página.\n",
    "\n",
    "2. **Análisis del contenido:** El contenido recibido es procesado para identificar las secciones relevantes. Esto se realiza mediante el análisis del DOM (Modelo de Objetos del Documento) de la página web.\n",
    "\n",
    "3. **Extracción de datos:** Los elementos de interés (como textos, imágenes o enlaces) son localizados y extraídos según las especificaciones definidas previamente.\n",
    "\n",
    "4. **Almacenamiento de datos:** La información recolectada se estructura y guarda en un formato adecuado, como CSV, JSON o bases de datos.\n",
    "\n",
    "### Aplicaciones del Web Scraping\n",
    "\n",
    "El web scraping es una herramienta poderosa y versátil. Algunas de sus aplicaciones más comunes incluyen: monitorización de precios, análisis de mercado, recopilación de noticias, Machine Learning, proyectos académicos (extracción de datos específicos para trabajos de investigación.)\n",
    "\n",
    "### Consideraciones Éticas y Legales\n",
    "\n",
    "Aunque el web scraping ofrece múltiples beneficios, es esencial tener en cuenta los aspectos legales y éticos relacionados con su uso. Algunos puntos importantes son:\n",
    "\n",
    "- **Respeto a los Términos de Servicio:** Muchos sitios web especifican en sus términos y condiciones las restricciones sobre el uso de crawlers y scrapers.\n",
    "\n",
    "- **Evitar sobrecargar los servidores:** Es fundamental diseñar soluciones que no generen un volumen excesivo de solicitudes al servidor, lo que podría causar problemas de rendimiento.\n",
    "\n",
    "- **Protección de datos:** Asegurarse de no recopilar información que infrinja las leyes de privacidad o derechos de autor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo en python para Web Scraping\n",
    "\n",
    "Se usaron las librerias BeautifulSoup requests y Scrapy para realizar un ejemplo de scraping al sitio web de BBC News."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "South Korea plane crash kills 179 with investigation into cause under way\n",
      "Video captures moments before South Korea plane crash\n",
      "Billionaire HBO creator Charles Dolan dies aged 98\n",
      "As Putin reaches 25 years in power, has he 'taken care of Russia'?\n",
      "Nigerians take to the streets for Calabar Carnival \n",
      "Video captures moments before South Korea plane crash\n",
      "Did bird strike contribute to South Korea plane crash? What we know so far\n",
      "New elections could take up to four years, Syria rebel leader says\n",
      "Chlamydia could make koalas extinct. Can a vaccine save them in time?\n",
      "Billionaire HBO creator Charles Dolan dies aged 98\n",
      "Lost city found by accident and a fly's brain mapped: 2024's scientific wins\n",
      "Quiz of the Year, Part 4: Why did 100 couples all say 'I do' together?\n",
      "Tourist killed in shark attack off Egyptian coast\n",
      "Trump sides with tech bosses in Maga fight over immigrant visas\n",
      "Azerbaijan urges Russia to accept blame for plane crash\n",
      "More to explore\n",
      "Maggie Smith, Liam Payne and the other famous people who died in 2024\n",
      "Inside a Syrian 'reconciliation centre' where Assad's soldiers give up their weapons\n",
      "UK and EU look to 2025 for reset, but with little room for trade-offs\n",
      "Growth of women in power grinds to near-halt in a mega-election year\n",
      "From Squid Game to Blackpink, how South Korea became a culture powerhouse\n",
      "Inside a Syrian 'reconciliation centre' where Assad's soldiers give up their weapons\n",
      "A year of extreme weather that challenged billions\n",
      "How feminism, not Bollywood, drew global audiences to Indian cinema in 2024\n",
      "UK and EU look to 2025 for reset, but with little room for trade-offs\n",
      "Most watched\n",
      "Video captures moments before South Korea plane crash\n",
      "Watch: Huge waves strike Peruvian coastline\n",
      "Azerbaijan plane survivors recall moments before crash\n",
      "Watch: Police officer dressed as the Grinch leads drug raid\n",
      "Footage shows survivors walking from crashed Azerbaijani plane\n",
      "Also in news\n",
      "Toddler nearly runs off cliff at Hawaii volcano\n",
      "School chaplain killed in shark attack on Australia's Great Barrier Reef\n",
      "Trump sides with tech bosses in Maga fight over immigrant visas\n",
      "Romeo and Juliet actress Olivia Hussey dies aged 73\n",
      "Watch: Huge waves strike Peruvian coastline\n",
      "School chaplain killed in shark attack on Australia's Great Barrier Reef\n",
      "Kiefer Sutherland grew up unaware of dad Donald's success\n",
      "Three migrants die attempting to cross Channel\n",
      "Trump sides with tech bosses in Maga fight over immigrant visas\n",
      "Most read\n",
      "South Korea plane crash kills 179 with investigation into cause under way\n",
      "Did bird strike contribute to South Korea plane crash? What we know so far\n",
      "New elections could take up to four years, Syria rebel leader says\n",
      "As Putin reaches 25 years in power, has he 'taken care of Russia'?\n",
      "Billionaire HBO creator Charles Dolan dies aged 98\n",
      "Notable deaths 2024\n",
      "Chlamydia could make koalas extinct. Can a vaccine save them in time?\n",
      "Rebel Wilson marries Ramona Agruma in Sydney ceremony\n",
      "Lost city found by accident and a fly's brain mapped: 2024's scientific wins\n",
      "Kiefer Sutherland grew up unaware of dad Donald's success\n",
      "Sport\n",
      "PDC World Darts Championship: Owen beats Evans in last third-round tie\n",
      "Ruthless Liverpool thrash West Ham to go eight points clear\n",
      "Thuram scores twice but Juventus held by Fiorentina\n",
      "Rabada sends SA into Test final with thrilling win\n",
      "Australia rally after Bumrah genius on riveting day\n",
      "Ruthless Liverpool thrash West Ham to go eight points clear\n",
      "Forest lend retro feel to table - here's why they can stay the course\n",
      "'We need help' - Guardiola targets January & says 'no chance' of title\n",
      "Thuram scores twice but Juventus held by Fiorentina\n",
      "Follow BBC on:\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "page = requests.get(\"https://www.bbc.com/news\")  # para obtener el html de la pagina\n",
    "soup = BeautifulSoup(page.text, \"html.parser\")  # para parsear el html\n",
    "\n",
    "# Extraer los titulares de las noticias\n",
    "headlines = soup.find_all('h2')\n",
    "\n",
    "# Imprimir los titulares\n",
    "for headline in headlines:\n",
    "    print(headline.get_text())\n",
    "\n",
    "    # Abrir un archivo CSV para escribir\n",
    "    with open('headlines.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=['headline'])\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Escribir los titulares en el archivo CSV\n",
    "        for headline in headlines:\n",
    "            writer.writerow({'headline': headline.get_text()})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapy \n",
    "\n",
    "Para usar Scrapy primero se creo un proyecto por terminal de la siguiente manera: `scrapy startproject noticias`. Lo cual genera un directorio.\n",
    "\n",
    "Luego se creo un spider en la ubicacion `noticias/noticias/spiders` el spider es un archivo de Python que se lo creo usando: `touch titulares_spider.py`  \n",
    "El archivo titulares_spider.py contiene el siguiente código:\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class TitularesSpider(scrapy.Spider):\n",
    "    name = \"titulares\"\n",
    "    start_urls = [\"https://www.bbc.com/news\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Selecciona los elementos que contienen los titulares\n",
    "        for article in response.css(\"div.gs-c-promo\"):\n",
    "            yield {\n",
    "                \"title\": article.css(\n",
    "                    \"h2::text\"\n",
    "                ).get(),  # Extrae el texto del titular.\n",
    "            }\n",
    "\n",
    "```\n",
    "\n",
    "Teniendo todo esto en cuenta se ejecuto el spider: `scrapy crawl titulares -o titulares.json`\n",
    "\n",
    "Esto generó un archivo .json que contiene los titulares extraídos.\n",
    "\n",
    "![](ADJUNTOS/titlejson.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se comprobó que los resultados obtenidos coinciden con el de la página web de BBC News.\n",
    "\n",
    "![](ADJUNTOS/bbcNews.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "El web scraping es una técnica eficaz para extraer y estructurar información de la web. Gracias a las herramientas disponibles, como BeautifulSoup y Scrapy en Python, es posible implementar soluciones potentes que facilitan la automatización de tareas repetitivas y la obtención de datos valiosos. Sin embargo, es crucial realizar estas actividades de manera ética y respetuosa con las normas establecidas, asegurando un equilibrio entre el aprovechamiento de los recursos web y el cumplimiento de los principios legales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliografía \n",
    "\n",
    "- Glez-Peña, D., Lourenço, A., López-Fernández, H., Reboiro-Jato, M., & Fdez-Riverola, F. (2014). Web scraping technologies in an API world. Briefings in bioinformatics, 15(5), 788-797. Obtenido de: [Web scraping technologies in an API world](https://academic.oup.com/bib/article/15/5/788/2422275)\n",
    "\n",
    "- Yuste, G. (2022, agosto 17). ¿Qué es web scraping? KeepCoding Bootcamps. https://keepcoding.io/blog/que-es-web-scraping/\n",
    "\n",
    "- Chanda, S. (2022, noviembre 28). Web Scraping Usando Python: Guía paso a paso. Geekflare Spain. https://geekflare.com/es/web-scraping-with-python/\n",
    "\n",
    "\n",
    "### Librerias usadas \n",
    "\n",
    "- BeautifulSoup: https://beautiful-soup-4.readthedocs.io/en/latest/\n",
    "\n",
    "- Scrapy: https://scrapy.org/ \n",
    "\n",
    "- Requests: https://docs.python-requests.org/en/latest/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
